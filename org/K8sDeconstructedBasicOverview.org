#+Title: Notes on Kubernetes Deconstructed

* Purpose                                                   :k8s:notes:video:
This is a collection of notes gathered from watching [[https://vimeo.com/245778144/4d1d597c5e][Kubernetes Deconstructed with Carson Anderson]]
* Container is:
  - Single service
  - All things about single application, runtime, disk
  - Should only be ONE application per service
* Pod is smallest resource grouping in Kubernetes
  - Should be a complimentary service like my app, prometheus exporter, envoy, data .... all things needed to run that one thing
  - Pod can be one or multiple containers
* Multiple pods together is deployment tool
  - Now I can have multiple instances of my service running
  - I can scale number of services up or down, this gives me redundancy
* Deployments tied together is a service
  - Service is an abstraction that will function as a load balancer for the pods. I give a deployment a name and an ip and that becomes the service.
  - I can take multiple pods, put them into a deployment. I can then put a service in front of that
  - Now if I change pods inside the deployment the service will automatically start routing traffic to the new pods as they become ready.
* Ingress
  - External routing to a service
  - external traffic comes into ingress, ingress points a specific url (traffic pattern) to a service

* Creating things in k8s
  - Talk to kluster api and provide credentials
  - kubectl is the tool we use for talking to cluster api


* Kubernetes is 5 core components
** kube api-server
*** Custom resources
    1. You can define things like mysql as part of the api service, ie now you can ~kubectl mysql~ the same way you do ~deploy/service/ingress~
    2. Aggregate things
*** Everything in the api service gets passed off to ETCD - https://etcd.io/ (the data store)
    1. etcd is a strongly consistent, distributed key-value store
    2. etcd is the state and event aware
    3. Really great at storing
** kube-scheduler
*** Matre-d, makes sure resources have a place to live, ie pods moved to nodes
*** Scheduler watches api server for any changes to pods, scheduler will try to assign each new pod to a node
*** Scheduler can have affinity defined, ie each new pod of this name goes into that node
*** scheduler can be made custom, ie you can watch events outside of k8's and have the scheduler respond on resources in the cluster based on these external things
** kube-Controller-manager
*** Brain of the system
*** Manages the various controllers in k8s example of 3 controllers is namespace-controller, deployment-controller, replicaset-controller
*** Easy to plug in custom controller, so have manager watch for specific events
*** example
    - We made a namespace
      - A controller made a service account for that namespace
      - A different controller added a default secret for that namespace
      - A controller pushed out the deployment
      - Another controller set up a replica set as defined in the deployment
      - Another controller created the pods inside the nodes
*** controller is the operating system of k8s ?
** Kubelet
*** This is the part that lives on each nodes and starts the actual pods
*** Will do health checks, ie check that pod is actually taking traffic etc
** Kube-Proxy
*** This is the main network component on each nodes
*** This makes services live on the actual nodes
* Network services
** pods each have a unique ip
** Pods live in nodes
** Nodes get CIDr
** Lots of network providers on a cluster, Rules
*** All pods can talk to all pods without a NAT
*** All nodes can communicate with all containers(pods) without a NAT
*** The IP a container thinks it has is the same IP everything else in the cluster thinks it has
* Services
** They have a name
** They have a port
** Services has a type, one of 3
*** LoadBalancer
    - This sits above your nodeport
    - This would be AWS ALB for instance
*** NodePort
    - This sits above clusterIP
    - A way to traverse from outside the cluster into the nodes, I have a service ip, but each node gets a port assigned, now if outside the cluster that ip gets hit iptable rules (kube-proxy) will forward requests to that port to the right nodes
*** ClusterIP
    - core service for k8s now everything in the cluster knows the ip for the service. In service you can reference that ip by its label, ie cache service once defined in services can be referenced by its name not ip, k8 will translate name into ip

* k8s cloud Admin
** Two types of machines, master and nodes
* Ingress-controller
** WYSIWYG
* Going into container specific details
** Namespaces
   - I can isolate based on pod, based on name and network namespace
   - In k8s we very much care about network namespace
** Control groups (CNAME)
   - Takes all CPU/RAM/resources and break it up into chunks
   - Brilliant part, this also audits all the resources
** Union file systems
   - Building file system efficiently
   - Breaking files into layer, sop first few layers would be everything the container needs, presented as one fs
   - The container can then add its own layer on top for its writing needs
   - This means that 5 containers can share 99% of a single file system and each put a layer specific to them on top
* Kubelet
** Docker does not have the concept of grouped containers (pods), kubelet helps us group docker containers together into pods
** Kubelet adds infra container and joins all the other containers in the pod into that linux network namespace
*** Kubelet hears about a pod, talks to CNI to create an ip and register a namespace for it, it will then talk to docker to create a infra container with that ip registered.
*** All additional containers in a pod will then be tied to that ip namespace so the individual containers in the pod can be talked to as one "resource"(pod)
* Logging:
** Default "kubectl logs <pod>
*** kubectl -> api service -> kubelet -> docker ->  streams all std-in/std-err back out to the user
