#+TITLE: Improved Startup

* Purpose
  Hold thoughtson how we can improve apisnoop's default environment from the start.  Not sure the right words forthis, but essentially improve the baseline of how apisnoop operates.
* Current Data
  - apisnoop can be added to a cluster with `apply raiinbow.yaml`
  - this will start up postgres and hasura and load in a single series of audit events from one bucket/job.
  - this bucket/job is hardcoded into the migrations file for hasura.
* Improvements to Data
  - The audit event bucket/job that is being loaded is transparent to any apisnooper.
  - Ideally, it is dynamically grabbed based on the last successful test run in testgrid.
  - This can also be overridden with a flag or config setting.
  - I think there should still only be one bucket/job added, as it acts as the basline, but perhaps there's a sources.yaml of sorts that lets you set what it should be.
  - If there is no --flag or yaml present, we query for the latest test run, ensuring it is a successful test run, and grab the bucket and job setting it to a postgres custom setting. 
  - our migration now uses the custom settings for its two arguments instead of being hard-coded.
* Current ergonomics
  (this relates to how you access the services and the data we make available).
  - You must go through the setup steps in our workflow.org, using our particular version of emacs.
  - You must open forward ports for postgres and hasura.
  - You can run ad-hoc queries in our org file that is sent to postgres and returns yr data.
  - you can find available views with ~\d+~ and learn more about a view with ~select * from view limit 0;~
  - It's possible, likely, to run the queries without using org, by accessing the cluster, running psql, and then writing the queries in this interpreter.

* Improvements to Ergonomics
  - You can get everything for apisnoop up and running on your audit-configured cluster by running `apply raiinbow.yaml`
    - We have our own steps available in workflow.org, but they are not mandatory for getting apisnoop working.
    - There's a small set of transparent pre-reqs (a cluster, auditing set for that cluster) but it doesn't matter if it was made with kind or minikube or what have you.
  - Upon applying, you can now go to ~cluster.url/apisnoop~, to access quick links
  - apisnoop on the cluster has four different views into the data:
    - our existing org-centric view
    - a graphiql explorer available at ~apisnoop/explore~
    - a help page that documents each pre-built view, its available columns, and example of use.
    - a stats page that shows the results of some pre-built views around testing, and links to the above views.

* Steps to Improve
  - develop networking to have apisnoop accessible through quick url on the cluster.
  - iterate setup steps until org is not required.
  - develop stats page and documentation (either as part of the graphql scheme in hasura, or as generated from our tables_and_views).
* Steps to go Beyond Improvement.
  - integrate sql results into go documentation from within org (looking up a function definition, for example, that hits the path given by an operationID)
  - integrate sql views with k8s reference documentation (might be as simple as building a url from api_operation_material results).
