#+TITLE: 88: Audit Sink Extended work
#+TODO: TODO IN-PROGRESS BLOCKED | TADA

* Purpose
This is a continuation of ticket 88, which has already been mostly done through caleb's work.  The ticket is focused on listening to when new events are added to our live_audit_events db and then running queries on how coverage ahs changed.  These queries require an operation_id to be added to the event table, which is not included in the event data.  So this file is focused on the work to get that id added and setup some good queries.

* Process
** TADA change audit_sink to insert into live_audit_event db instead
   CLOSED: [2019-11-09 Sat 02:02]
   This is so it's easier to delete everything from a db on runs, then if everything was going into our raw audit events.
   this also sets it up nicely for when we wanna do a full e2e coverage diff report, where the result of that will go into our raw audit events.
   So we will need to first make the table and then the view.
** TADA Create a live_audit_event table
   CLOSED: [2019-11-09 Sat 02:03]
   this is identical to raw_audit_event so we can essentially copy and paste.
** TADA create trigger for when new event added into live_audit_event
   CLOSED: [2019-11-09 Sat 02:52]
   For this we want to have an iteration loop where we:
   - insert data into live_audit_event
   - run a query to return the op_id of this data
   - delete the entry
   This will let us see if the trigger is workign and repeatedly do it.  So I've created an iteration loop heading as part of this ticket.
   
   The basic trigger will take the event and create an operation id that says success!  this will show _a_ trigger is working.  the we can re-implement to make the op_id something actually useful.
   
   #+begin_src sql-mode :results silent
     CREATE FUNCTION add_op_id() RETURNS TRIGGER as $add_op_id$
     BEGIN
       NEW.operation_id := 'success';
       RETURN NEW;
     END;
     $add_op_id$ LANGUAGE plpgsql;
   #+end_src
   
   #+begin_src sql-mode :results silent
     CREATE TRIGGER add_op_id BEFORE INSERT ON live_audit_event
       FOR EACH ROW EXECUTE PROCEDURE add_op_id();
   #+end_src
   
   Running through iteration loop we can verify it works!

** TADA setup trigger to grab our open api spec and find op_id based on it.
   CLOSED: [2019-11-09 Sat 22:43]
   So now we want a slightly more involved function.  Here we want to write it in python, and check for a global variable.
   
   We will bring in the code from our load_audit_event function as this is basically the same, just for a single event instead of looping.

#+NAME: deep_merge
#+BEGIN_SRC python :tangle no
  from copy import deepcopy
  from functools import reduce

  def deep_merge(*dicts, update=False):
      """
      Merges dicts deeply.
      Parameters
      ----------
      dicts : list[dict]
          List of dicts.
      update : bool
          Whether to update the first dict or create a new dict.
      Returns
      -------
      merged : dict
          Merged dict.
      """
      def merge_into(d1, d2):
          for key in d2:
              if key not in d1 or not isinstance(d1[key], dict):
                  d1[key] = deepcopy(d2[key])
              else:
                  d1[key] = merge_into(d1[key], d2[key])
          return d1

      if update:
          return reduce(merge_into, dicts[1:], dicts[0])
      else:
          return reduce(merge_into, dicts, {})
#+END_SRC

#+NAME: load_openapi_spec
#+BEGIN_SRC python :tangle no
  def load_openapi_spec(url):
      cache=defaultdict(dict)
      openapi_spec = {}
      openapi_spec['hit_cache'] = {}

      swagger = requests.get(url).json()
      for path in swagger['paths']:
          path_data = {}
          path_parts = path.strip("/").split("/")
          path_len = len(path_parts)
          path_dict = {}
          last_part = None
          last_level = None
          current_level = path_dict
          for part in path_parts:
              if part not in current_level:
                  current_level[part] = {}
              last_part=part
              last_level = current_level
              current_level = current_level[part]
          for method, swagger_method in swagger['paths'][path].items():
              if method == 'parameters':
                  next
              else:
                  current_level[method]=swagger_method.get('operationId', '')
          cache = deep_merge(cache, {path_len:path_dict})
      openapi_spec['cache'] = cache
      #import ipdb; ipdb.set_trace(context=60)
      return openapi_spec
#+END_SRC

#+NAME: find_operation_id
#+BEGIN_SRC python :tangle no
  def find_operation_id(openapi_spec, event):
    verb_to_method={
      'get': 'get',
      'list': 'get',
      'proxy': 'proxy',
      'create': 'post',
      'post':'post',
      'put':'post',
      'update':'put',
      'patch':'patch',
      'connect':'connect',
      'delete':'delete',
      'deletecollection':'delete',
      'watch':'get'
    }
    method=verb_to_method[event['verb']]
    url = urlparse(event['requestURI'])
    # 1) Cached seen before results
    if url.path in openapi_spec['hit_cache']:
      if method in openapi_spec['hit_cache'][url.path].keys():
        return openapi_spec['hit_cache'][url.path][method]
    uri_parts = url.path.strip('/').split('/')
    if 'proxy' in uri_parts:
        uri_parts = uri_parts[0:uri_parts.index('proxy')]
    part_count = len(uri_parts)
    try: # may have more parts... so no match
        cache = openapi_spec['cache'][part_count]
    except Exception as e:
      plpy.warning("part_count was:" + part_count)
      plpy.warning("spec['cache'] keys was:" + openapi_spec['cache'])
      raise e
    #  import ipdb; ipdb.set_trace(context=60)
    last_part = None
    last_level = None
    current_level = cache
    for idx in range(part_count):
      part = uri_parts[idx]
      last_level = current_level
      if part in current_level:
        current_level = current_level[part] # part in current_level
      elif idx == part_count-1:
        if part == 'metrics': # we aren't collecting metrics for now
          return None
        #   elif part == '': # The last V
        #     current_level = last_level
        #       else:
        variable_levels=[x for x in current_level.keys() if '{' in x] # vars at current(final) level?
        if len(variable_levels) > 1:
          raise "If we have more than one variable levels... this should never happen."
          # import ipdb; ipdb.set_trace(context=60)
        next_level=variable_levels[0] # the var is the next level
        current_level = current_level[next_level] # variable part is final part
      else:
        next_part = uri_parts[idx+1]
        variable_levels={next_level:next_part in current_level[next_level].keys() for next_level in [x for x in current_level.keys() if '{' in x]}  
        if not variable_levels: # there is no match
          if 'example.com' in part:
            return None
          elif 'kope.io' in part:
            return None
          elif 'snapshot.storage.k8s.io' in part:
            return None
          elif 'metrics.k8s.io' in part:
            return None
          elif 'wardle.k8s.io' in part:
            return None
          elif ['openapi','v2'] == uri_parts: # not part our our spec
            return None
          else:
            print(url.path)
            return None
        next_level={v: k for k, v in variable_levels.items()}[True]
        current_level = current_level[next_level] #coo
    try:
      op_id=current_level[method]
    except Exception as err:
      plpy.warning("method was:" + method)
      plpy.warning("current_level keys:" + current_level.keys())
      raise err
    #   import ipdb; ipdb.set_trace(context=60)
    if url.path not in openapi_spec['hit_cache']:
      openapi_spec['hit_cache'][url.path]={method:op_id}
    else:
      openapi_spec['hit_cache'][url.path][method]=op_id
    return op_id
#+END_SRC

   #+NAME: Drop Function
   #+begin_src sql-mode :results silent
   DROP FUNCTION add_op_id CASCADE;
   #+end_src

   #+NAME: Create Function
   #+begin_src sql-mode :results silent :noweb yes
     CREATE FUNCTION add_op_id() RETURNS TRIGGER as 
     $$
       import json
       from urllib.request import urlopen, urlretrieve
       import os
       import re
       from bs4 import BeautifulSoup
       import subprocess
       import time
       import glob
       from tempfile import mkdtemp
       from string import Template
       from urllib.parse import urlparse
       import requests
       import hashlib
       from collections import defaultdict
       import json
       import csv
       import sys
       <<deep_merge>>
       <<load_openapi_spec>>
       <<find_operation_id>>
       if "spec" not in GD:
           GD["spec"] = "good times"
# load_openapi_spec('https://raw.githubusercontent.com/kubernetes/kubernetes/7d13dfe3c34f44/api/openapi-spec/swagger.json')
       # spec = GD["spec"]
       # event = json.loads(TD["new"]["data"])
       if TD["new"]["operation_id"] is None:
           TD["new"]["operation_id"] = GD["spec"];
         # find_operation_id(spec, event);
       return "MODIFY";
     $$ LANGUAGE plpython3u;
   #+end_src
   
   #+NAME: Create Trigger
   #+begin_src sql-mode :results silent
     CREATE TRIGGER add_op_id BEFORE INSERT ON raw_audit_event
       FOR EACH ROW EXECUTE PROCEDURE add_op_id();
   #+end_src
   
   If we run through the iteration loop, we can see a correct op_id applied only if there is no existing op_id. Great!
** TADA setup a coverage report specifically for the live audit_events.
   CLOSED: [2019-11-09 Sat 22:44]
   We actually don't need to do this,w e can add things to our raw_audit_event and then filter using a where clause (we can also delee using a where clause)
** TADA Update tables_and_views_bot to include this trigger.
   CLOSED: [2019-11-09 Sat 23:47]
** TODO Verify everything works.
* Iteration Loop for Trigger


  #+NAME: Insert Entry to raw_audit_event
  #+begin_src sql-mode :noweb yes :results silent
    INSERT INTO raw_audit_event( 
    bucket,
    job,
    audit_id,
    stage,
    event_verb,
    request_uri,
    data
    )
    VALUES(
      'apisnoop'
      ,'live'
      ,'audit11231'
      ,'bigstage'
      ,'create'
      ,'https://itworks.com'
      , (select data from raw_audit_event where operation_id is not null limit 1)
    )
      ;
  #+end_src
  
  
  #+NAME: Check op_id and other data from raw_audit_event
  #+begin_src sql-mode 
  select audit_id, operation_id from raw_audit_event where bucket = 'apisnoop';
  #+end_src

  #+RESULTS: Check op_id and other data from raw_audit_event
  #+begin_src sql-mode
    audit_id  |  operation_id  
  ------------+----------------
   audit11231 | readCoreV1Node
  (1 row)

  #+end_src

  #+NAME: Delete Entries from raw_audit_event
  #+begin_src sql-mode :results silent
  DELETE FROM raw_audit_event where bucket = 'apisnoop';
  #+end_src

* Footnotes
** Connect to db
   
  connect with this elisp block
  #+NAME: Connect org to postgres
  #+BEGIN_SRC emacs-lisp :results silent
    (sql-connect "apisnoop" (concat "*SQL: postgres:data*"))
  #+END_SRC
  
  check your connection with \conninfo.  If successful you should see this message in your minibuffer
  : You are connected to database "apisnoop" as user "apisnoop" on host "localhost" at port "10041".

  #+NAME: Test Connection
  #+BEGIN_SRC sql-mode :results silent
  \conninfo
  #+END_SRC
