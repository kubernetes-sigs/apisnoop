#+TITLE: Postgres Code, Deployment, and Reference Materials
#+TODO: TODO(t) IN-PROGRESS(i) WAITING(w) | DONE(d)

* Introduction
  A database which stores the converted OpenAPI spec and processed auditlogs, so coverage can be investigated via API endpoints.

** TODO This database isn't actually used for anything, we can go forth and delete it
* App
** Dockerfile
#+begin_src dockerfile :tangle ./app/Dockerfile
FROM postgres:12.0
 MAINTAINER Hippie Hacker <hh@ii.coop>
 RUN apt-get update \
   && apt-get install -y --no-install-recommends \
   postgresql-plpython3-12 \
   postgresql-12-plsh \
   python3-bs4\
   python3-psycopg2\
   python3-ipdb\
   python3-requests \
   wget \
   make \
   gcc \
   libc6-dev \
   curl \
   jq \
   git \
   software-properties-common \
   apt-transport-https
 #  && rm -rf /var/lib/apt/lists/*

 # RUN curl -L https://dl.google.com/go/go1.12.4.linux-amd64.tar.gz \
 #   | tar -C /usr/local -xzf - \
 #   && echo 'export PATH=$PATH:/usr/local/go/bin' \
 #   > /etc/profile.d/usr-local-go-path.sh \
 #   && echo 'export PATH=$PATH:$HOME/go/bin' \
 #   > /etc/profile.d/homedir-go-path.sh
 # RUN . /etc/profile.d/usr-local-go-path.sh \
 #   && . /etc/profile.d/homedir-go-path.sh \
 #   && go get github.com/golangci/gofmt/gofmt \
 #   && go get -u golang.org/x/lint/golint \
 #   && go get golang.org/x/tools/cmd/goimports \
 #   && go get github.com/jgautheron/goconst/cmd/goconst \
 #   && go get github.com/jgautheron/usedexports \
 #   && go get -u github.com/kisielk/errcheck \
 #   && go get github.com/ii/apisnoopregexp \
 #   && cd ~/go/src/github.com/ii/apisnoopregexp \
 #   && make install \
 #   && cp ~/go/bin/rmatch /usr/local/bin
 COPY ./initdb /docker-entrypoint-initdb.d
 COPY ./snoopUtils.py /usr/local/lib/python3.7/dist-packages/snoopUtils.py
 HEALTHCHECK --interval=10s --timeout=5s --start-period=5s --retries=5 \
   CMD ["pg_isready", "-U ", "apisnoop"] || exit 1
# RUN sed -i -e"s/^#logging_collector = off.*$/logging_collector = on/" /var/lib/postgresql/data/postgresql.conf
#+end_src

** cloudbuild.yaml
#+begin_src yaml :tangle ./app/cloudbuild.yaml
steps:
  - name: gcr.io/cloud-builders/docker
    args: ['build', '-t', 'gcr.io/$PROJECT_ID/postgres:$_GIT_TAG',
           '--build-arg', 'IMAGE_ARG=gcr.io/$PROJECT_ID/postgres:$_GIT_TAG',
           '.']
    # dir: .
    # dir: apps/auditlogger
substitutions:
  _GIT_TAG: '12345'
images:
  - 'gcr.io/$PROJECT_ID/postgres:$_GIT_TAG'
options:
  substitution_option: 'ALLOW_LOOSE'
#+end_src

** apisnoop_db.sql
#+begin_src sql-mode :tangle ./app/apisnoop_db.sql   
-- init-apisnoop-db.sh

--  #+NAME: init apisnoop db

-- ERROR:  database "apisnoop" already exists
-- create database apisnoop;
-- create user myuser with encrypted password 'mypass';
grant all privileges on database apisnoop to apisnoop;
create role dba with superuser noinherit;
grant dba to apisnoop;
\connect apisnoop
-- we generate uuids
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
-- we write python functions
CREATE EXTENSION IF NOT EXISTS plpython3u;
-- we write shell functions
CREATE EXTENSION IF NOT EXISTS plsh;
-- regex indexes required trgm
CREATE EXTENSION IF NOT EXISTS pg_trgm;
-- paths need an index too
CREATE EXTENSION IF NOT EXISTS ltree;
-- hasura needs hash functions
CREATE EXTENSION IF NOT EXISTS pgcrypto;
-- hasura db catalog and views
CREATE SCHEMA IF NOT EXISTS hdb_catalog;
CREATE SCHEMA IF NOT EXISTS hdb_views;
-- make the user an owner of system schemas
ALTER SCHEMA hdb_catalog OWNER TO apisnoop;
ALTER SCHEMA hdb_views OWNER TO apisnoop;
GRANT SELECT ON ALL TABLES IN SCHEMA information_schema TO apisnoop;
GRANT SELECT ON ALL TABLES IN SCHEMA pg_catalog TO apisnoop;
GRANT USAGE ON SCHEMA public TO apisnoop;
GRANT ALL ON ALL TABLES IN SCHEMA public TO apisnoop;
GRANT ALL ON ALL SEQUENCES IN SCHEMA public TO apisnoop;
GRANT pg_execute_server_program TO apisnoop;
#+end_src

** snoopUtils: Our Python Functions
  :PROPERTIES:
  :header-args:python: :tangle "./app/snoopUtils.py" :noweb yes :comments none :session "snoopUtils"
  :END:
*** Our Imports
  #+NAME: snoopUtils Imports
  #+begin_src python :results silent
    import os
    import json
    from urllib.request import urlopen, urlretrieve
    from string import Template
    import requests
    import re
    from copy import deepcopy
    from functools import reduce
    from collections import defaultdict
    from urllib.parse import urlparse
    from bs4 import BeautifulSoup
    import subprocess
    import warnings
    from tempfile import mkdtemp
    import time
    import glob
  #+end_src
*** Our Constants
  #+NAME: snoopUtil Constants
  #+begin_src python :results silent
    GCS_LOGS="https://storage.googleapis.com/kubernetes-jenkins/logs/"
    DEFAULT_BUCKET="ci-kubernetes-gci-gce"
    K8S_GITHUB_RAW= "https://raw.githubusercontent.com/kubernetes/kubernetes/"
    # Why do we have to had this?
    # The k8s VERB (aka action) mapping to http METHOD
    # Our audit logs do NOT contain any mention of METHOD, only the VERB
    VERB_TO_METHOD={
        'get': 'get',
        'list': 'get',
        'proxy': 'proxy',
        'create': 'post',
        'post':'post',
        'put':'post',
        'update':'put',
        'patch':'patch',
        'connect':'connect',
        'delete':'delete',
        'deletecollection':'delete',
        'watch':'get'
    }
    IGNORED_ENDPOINTS=[
        'metrics',
        'readyz',
        'livez',
        'healthz',
        'finalize', # recently came up, am unsure what this or finalize-api mean or if we should be tracking it
        'finalize-api',
        'status' # hunch is this was a removed endpoint...but does not show in swagger.json
    ]
    # TODO: answer question: what are finalizers and the finalize-api?
    DUMMY_URL_PATHS =[
        'example.com',
        'kope.io',
        'snapshot.storage.k8s.io',
        'discovery.k8s.io',
        'metrics.k8s.io',
        'wardle.k8s.io'
    ]
  #+end_src

*** get_json  Definition
  #+NAME: get_json
  #+begin_src python :results silent
    def get_json(url):
        """Given a json url path, return json as dict"""
        body = urlopen(url).read()
        data = json.loads(body)
        return data
  #+end_src

*** determine_bucket_job Definition

  #+begin_src python :results silent
    def determine_bucket_job(custom_bucket=None, custom_job=None):
        """return tuple of bucket, job, using latest succesfful job of default bucket if no custom bucket or job is given"""
        #establish bucket we'll draw test results from.
        baseline_bucket = os.environ['APISNOOP_BASELINE_BUCKET'] if 'APISNOOP_BASELINE_BUCKET' in os.environ.keys() else 'ci-kubernetes-e2e-gci-gce'
        bucket =  baseline_bucket if custom_bucket is None else custom_bucket
        #grab the latest successful test run for our chosen bucket.
        testgrid_history = get_json(GCS_LOGS + bucket + "/jobResultsCache.json")
        latest_success = [x for x in testgrid_history if x['result'] == 'SUCCESS'][-1]['buildnumber']
        #establish job
        baseline_job = os.environ['APISNOOP_BASELINE_JOB'] if 'APISNOOP_BASELINE_JOB' in os.environ.keys() else latest_success
        job = baseline_job if custom_job is None else custom_job
        return (bucket, job)

  #+end_src

*** fetch_swagger Definition
  :PROPERTIES:
  :header-args:python: :tangle no :comments none
  :END:
  This function is designed so, given just a bucket and a job, we can fetch a swagger.json file from github at the job's exact commit, and prepare it as a dict perfect for passing along to our load_swagger function.

  The structure of it is as so:

  #+NAME: define fetch_swagger
  #+begin_src python :tangle "./app/snoopUtils.py" :session snoopUtils :results silent
    def fetch_swagger(bucket, job):
        """fetches swagger for given bucket and job and returns it, and its appropariate metadata, in a dict"""
        <<grab metadata for bucket job>>
        <<determine commit hash from metadata>>
        <<grab swagger at commit hash>>
        <<return swagger and metadata>>

  #+end_src

  All the metadata we need can be found in the finished.json file for that bucket and job

  #+NAME: grab metadata for bucket job
  #+begin_src python
    metadata_url = ''.join([GCS_LOGS, bucket, '/', job, '/finished.json'])
    metadata = json.loads(urlopen(metadata_url).read().decode('utf-8'))
  #+end_src

  Within the version of the metadata is the commit hash that triggered this particular job.  We want to pull the swagger from the right period as the API can change quickly, for example an endpoint moving from alpha to beta to stable.
  #+NAME: determine commit hash from metadata 
  #+begin_src python
    commit_hash = metadata["version"].split("+")[1]
  #+end_src

  Then we grab the raw swagger.json from the k8s github, which lets us fetch the code from a particular hash.
  #+NAME:grab swagger at commit hash
  #+begin_src python
    swagger_url =  ''.join([K8S_GITHUB_RAW, commit_hash, '/api/openapi-spec/swagger.json'])
    swagger = json.loads(urlopen(swagger_url).read().decode('utf-8')) # may change this to ascii
  #+end_src

  Our bucket_job_swagger table will want to know the bucket, job, metadata, and swagger so we put them all into a dict, and we done!

  #+NAME: return swagger and metadata
  #+begin_src python
    return (swagger, metadata, commit_hash);
  #+end_src

*** deep_merge Definition

  Current understanding: 
  - this is a recursive function for merging two dicts.
  - We are doing a reduce starting with an empty dict.
  - We have merge into that will be comparing the two dicts in a list
  - for each key in the second dict (d2) we see if that key exists in first dict d1
  - if it does, we check if that key is also a dict
  - if both true, we merge the two keys together, by stepping into both and recursivel.y checking again
  - If not, we copy all of d2[key] into d1[key]

  I'd like to show an example, but it seems to be returning nothing....a bit odd
  Where does this get used?  Is it a  part of the cache?
    #+NAME: deep_merge example
    #+begin_src python :tangle no :results output
      d1 = {"a": "", "b": {"b1": "cool"}}
      d2 = {"a": {"a1": "fun"}, "b": {"b1": "awesome"}, "c": "sweet"}
      dicts = [d1, d2]
    #+end_src

    #+NAME: Define merge_into
    #+begin_src python
      def merge_into(d1, d2):
          for key in d2:
              if key not in d1 or not isinstance(d1[key], dict):
                  d1[key] = deepcopy(d2[key])
              else:
                  d1[key] = merge_into(d1[key], d2[key])
          return d1

    #+end_src


  #+NAME: Define deep_merge
  #+BEGIN_SRC python
    def deep_merge(*dicts, update=False):
        if update:
            return reduce(merge_into, dicts[1:], dicts[0])
        else:
            return reduce(merge_into, dicts, {})

  #+END_SRC

  #+RESULTS: Define deep_merge
  #+begin_src python
  #+end_src

*** get_html definition
  #+NAME: get_html 
  #+begin_src python
    def get_html(url):
        """return html content of given url"""
        html = urlopen(url).read()
        soup = BeautifulSoup(html, 'html.parser')
        return soup

  #+end_src
*** download_url_to_path definition
  #+NAME: download_url_to_path
  #+begin_src python
    def download_url_to_path(url, local_path, dl_dict):
        """
        downloads contents to local path, creating path if needed,
        then updates given downloads dict.
        """
        local_dir = os.path.dirname(local_path)
        if not os.path.isdir(local_dir):
            os.makedirs(local_dir)
        if not os.path.isfile(local_path):
            process = subprocess.Popen(['wget', '-q', url, '-O', local_path])
            dl_dict[local_path] = process
  #+end_src
*** get_all_auditlog_links
  #+begin_src python
    def get_all_auditlog_links(au):
        """
        given an artifacts url, au, return a list of all
        audit.log.* within it.
        (some audit.logs end in .gz)
        """
        soup = get_html(au)
        master_link = soup.find(href=re.compile("master"))
        master_soup = get_html("https://gcsweb.k8s.io" + master_link['href'])
        return master_soup.find_all(href=re.compile("audit.log"))

  #+end_src
*** load_openapi_spec Definition
  This loads just the paths portion of the open_api spec into a dictionary made up of paths and methods...with each part of the path its own dict ... this also seems to be recursive but am not fully tracking what the python is doing.  its iterating over each path, but does it create a flat dict at the end, or does each path have its smaller parts within it?  so 

  #+NAME: load_openapi_spec
  #+BEGIN_SRC python 
    def load_openapi_spec(url):
        # Usually, a Python dictionary throws a KeyError if you try to get an item with a key that is not currently in the dictionary.
        # The defaultdict in contrast will simply return an empty dict.
        cache=defaultdict(dict)
        openapi_spec = {}
        openapi_spec['hit_cache'] = {}
        swagger = requests.get(url).json()
        # swagger contains other data, but paths is our primary target
        for path in swagger['paths']:
            # parts of the url of the 'endpoint'
            path_parts = path.strip("/").split("/")
            # how many parts?
            path_len = len(path_parts)
            # current_level = path_dict  = {}
            last_part = None
            last_level = None
            path_dict = {}
            current_level = path_dict
            # look at each part of the url/path
            for part in path_parts:
                # if the current level doesn't have a key (folder) for this part, create an empty one
                if part not in current_level:
                    current_level[part] = {}
                    # last_part will be this part, unless there are more parts 
                    last_part=part
                    # last_level will be this level, unless there are more levels
                    last_level = current_level
                    # current_level will be this this 'folder/dict', this might be empty
                    # /api will be the top level v. often, and we only set it once
                    current_level = current_level[part]
            # current level is now pointing to the inmost url part hash
            # now we iterate through the http methods for this path/endpoint
            for method, swagger_method in swagger['paths'][path].items():
                # If the method is parameters, we don't look at it
                # think this method is only called to explore with the dynamic client
                if method == 'parameters':
                    next
                else:
                    # for the nested current_level (end of the path/url) use the method as a lookup to the operationId
                    current_level[method]=swagger_method.get('operationId', '')
                    # cache = {}
                    # cache = {3 : {'/api','v1','endpoints'} 
                    # cache = {3 : {'/api','v1','endpoints'} {2 : {'/api','v1'} 
                    # cache uses the length of the path to only search against other paths that are the same length
                    cache = deep_merge(cache, {path_len:path_dict})
                    openapi_spec['cache'] = cache
        return openapi_spec
  #+END_SRC
*** format_proxy_endpoint Definition
  This is used as part of our find_operation_id.  Proxy endpoints will have an endpoint structure  
  like so:
  : api/v1/proxy/arg1/arg2/arg3/argn+1
  The args given to proxy can be substantially long, and we aren't too concerned with them, but still want to be able to map a proxy endpoint to its open api spec.  And so we format the uri_parts to be the path with all arguments given to proxy collapsed into a single string.o

  #+NAME: format_uri_parts_for_proxy
  #+begin_src python
    def format_uri_parts_for_proxy(uri_parts):
        formatted_parts=uri_parts[0:uri_parts.index('proxy')]
        formatted_parts.append('proxy')
        formatted_parts.append('/'.join(
            uri_parts[uri_parts.index('proxy')+1:]))
        return formatted_parts
  #+end_src
*** find_operation_id Definition
#+NAME: find_operation_id
#+BEGIN_SRC python
  def find_operation_id(openapi_spec, event):
    method=VERB_TO_METHOD[event['verb']]
    url = urlparse(event['requestURI'])
    # 1) Cached seen before results
    # Is the URL in the hit_cache?
    if url.path in openapi_spec['hit_cache']:
      # Is the method for this url cached?
      if method in openapi_spec['hit_cache'][url.path].keys():
        # Useful when url + method is already hit multiple times in an audit log
        return openapi_spec['hit_cache'][url.path][method]
      # part of the url of the http/api request
    uri_parts = url.path.strip('/').split('/')
    # IF we git a proxy component, the rest of this is just parameters and we don't "count" them
    if 'proxy' in uri_parts:
      uri_parts = format_uri_parts_for_proxy(uri_parts)
      # We index our cache primarily on part_count
    part_count = len(uri_parts)
    # INSTEAD of try: except: maybe look into if cache has part count and complain explicitely with a good error
    try: # may have more parts... so no match
      # If we hit a length / part_count that isn't in the APISpec... this an invalid api request
      # our load_openapispec should populate all possible url length in our cache
        cache = openapi_spec['cache'][part_count]
    except Exception as e:
      # If you hit here, you are debugging... hence the warnings
      warnings.warn("part_count was:" + part_count)
      warnings.warn("spec['cache'] keys was:" + openapi_spec['cache'])
      raise e
    last_part = None
    last_level = None
    current_level = cache
    for idx in range(part_count):
      part = uri_parts[idx]
      last_level = current_level
      if part in current_level:
        current_level = current_level[part] # part in current_level
      elif idx == part_count-1:
        if part in IGNORED_ENDPOINTS or 'discovery.k8s.io' in uri_parts:
          return None
        variable_levels=[x for x in current_level.keys() if '{' in x] # vars at current(final) level?
        # If at some point in the future we have more than one... this will let un know
        if len(variable_levels) > 1:
          raise "If we have more than one variable levels... this should never happen."
        # inspect that variable_levels is not zero in length.  This indicates some new, spec-less uri
        if not variable_levels:
          print("NOTICE: uri part found that is not in apis spec.")
          print("URI: " + "/".join(uri_parts))
          return none
        variable_level=variable_levels[0] # the var is the next level
        # TODO inspect that variable level is a key for current_level
        current_level = current_level[variable_level] # variable part is final part
      else:
        next_part = uri_parts[idx+1]
        # TODO reduce this down to , find the single next level with a "{" in it 
        variable_levels=[x for x in current_level.keys() if '{' in x]
        if not variable_levels: # there is no match
          if part in DUMMY_URL_PATHS or uri_parts == ['openapi', 'v2']: #not part of our spec
            return None
          else:
            # TODO this is NOT valid, AND we didn't plan for it
            print(url.path)
            return None
        next_level=variable_levels[0]
        # except Exception as e: # TODO better to not use try/except (WE DON"T HAVE ANY CURRENT DATA")
        current_level = current_level[next_level] #coo
    try:
      op_id=current_level[method]
    except Exception as err:
      warnings.warn("method was:" + method)
      warnings.warn("current_level keys:" + current_level.keys())
      raise err
    if url.path not in openapi_spec['hit_cache']:
      openapi_spec['hit_cache'][url.path]={method:op_id}
    else:
      openapi_spec['hit_cache'][url.path][method]=op_id
    return op_id
#+END_SRC
*** download_and_process_auditlogs definitions
  #+NAME: download_and_process_auditlogs
  #+begin_src  python
    def download_and_process_auditlogs(bucket,job):
        """
        Grabs all audits logs available for a given bucket/job, combines them into a
        single audit log, then returns the path for where the raw combined audit logs are stored.
        The processed logs are in json, and include the operationId when found.
        """
        # BUCKETS_PATH = 'https://storage.googleapis.com/kubernetes-jenkins/logs/'
        ARTIFACTS_PATH ='https://gcsweb.k8s.io/gcs/kubernetes-jenkins/logs/'
        K8S_GITHUB_REPO = 'https://raw.githubusercontent.com/kubernetes/kubernetes/'
        downloads = {}
        # bucket_url = BUCKETS_PATH + bucket + '/' + job + '/'
        artifacts_url = ARTIFACTS_PATH + bucket + '/' +  job + '/' + 'artifacts'
        download_path = mkdtemp( dir='/tmp', prefix='apisnoop-' + bucket + '-' + job ) + '/'
        combined_log_file = download_path + 'audit.log'
        swagger, metadata, commit_hash = fetch_swagger(bucket, job)

        # download all metadata
        # job_metadata_files = [
        #     'finished.json',
        #     'artifacts/metadata.json',
        #     'artifacts/junit_01.xml',
        #     'build-log.txt'
        # ]
        # for jobfile in job_metadata_files:
        #     download_url_to_path( bucket_url + jobfile,
        #                           download_path + jobfile, downloads )

        # download all logs
        log_links = get_all_auditlog_links(artifacts_url)
        for link in log_links:
            log_url = link['href']
            log_file = download_path + os.path.basename(log_url)
            download_url_to_path( log_url, log_file, downloads)

        # Our Downloader uses subprocess of curl for speed
        for download in downloads.keys():
            # Sleep for 5 seconds and check for next download
            while downloads[download].poll() is None:
                time.sleep(5)

        # Loop through the files, (z)cat them into a combined audit.log
        with open(combined_log_file, 'ab') as log:
            for logfile in sorted(
                    glob.glob(download_path + '*kube-apiserver-audit*'), reverse=True):
                if logfile.endswith('z'):
                    subprocess.run(['zcat', logfile], stdout=log, check=True)
                else:
                    subprocess.run(['cat', logfile], stdout=log, check=True)

        # Process the resulting combined raw audit.log by adding operationId
        swagger_url = K8S_GITHUB_REPO + commit_hash + '/api/openapi-spec/swagger.json'
        openapi_spec = load_openapi_spec(swagger_url)
        infilepath=combined_log_file
        outfilepath=combined_log_file+'+opid'
        with open(infilepath) as infile:
            with open(outfilepath,'w') as output:
                for line in infile.readlines():
                    event = json.loads(line)
                    event['operationId']=find_operation_id(openapi_spec,event)
                    output.write(json.dumps(event)+'\n')
        return outfilepath
  #+end_src
*** json_to_sql definitions
  #+NAME: json_to_sql
  #+begin_src  python
    def json_to_sql(bucket,job,auditlog_path):
        """
          Turns json+audits into load.sql
        """
        import_number = job[-7:]
        try:
            sql = Template("""
    CREATE TEMPORARY TABLE raw_audit_event_import${import_number}(data jsonb not null) ;
    COPY raw_audit_event_import${import_number} (data)
    FROM '${audit_logfile}' (DELIMITER e'\x02', FORMAT 'csv', QUOTE e'\x01');

    INSERT INTO audit_event(bucket, job,
                            audit_id, stage,
                            event_verb, request_uri,
                            operation_id, event_level,
                            api_version, useragent,
                            test_hit, conf_test_hit,
                            event_user, object_namespace,
                            object_type, object_group,
                            object_ver, source_ips,
                            annotations, request_object,
                            response_object, response_status,
                            stage_timestamp, request_received_timestamp,
                            data)

    SELECT '${bucket}',
            '${job}',
            (raw.data ->> 'auditID'),
            (raw.data ->> 'stage'),
            (raw.data ->> 'verb'),
            (raw.data ->> 'requestURI'),
            (raw.data ->> 'operationId'),
            (raw.data ->> 'level') as event_level,
            (raw.data ->> 'apiVersion') as api_version,
            (raw.data ->> 'userAgent') as useragent,
            ((raw.data ->> 'userAgent') like 'e2e.test%') as test_hit,
            ((raw.data ->> 'userAgent') like '%[Conformance]%') as conf_test_hit,
            (raw.data -> 'user') as event_user,
            (raw.data #>> '{objectRef,namespace}') as object_namespace,
            (raw.data #>> '{objectRef,resource}') as object_type,
            (raw.data #>> '{objectRef,apiGroup}') as object_group,
            (raw.data #>> '{objectRef,apiVersion}') as object_ver,
            (raw.data -> 'sourceIPs') as source_ips,
            (raw.data -> 'annotations') as annotations,
            (raw.data -> 'requestObject') as request_object,
            (raw.data -> 'responseObject') as response_object,
            (raw.data -> 'responseStatus') as response_status,
            (raw.data ->> 'stageTimestamp') as stage_timestamp,
            (raw.data ->> 'requestReceivedTimestamp') as request_received_timestamp,
            raw.data
      FROM raw_audit_event_import${import_number} raw;
            """).substitute(
                  import_number=import_number,
                audit_logfile = auditlog_path,
                bucket = bucket,
                job = job
            )
            return sql
        except:
            return "something unknown went wrong"
  #+end_src
*** insert_audits_into_db definition                                  :notes:
This is good for notes, but not used 
  #+NAME: insert_audits_into_db
  #+begin_src python :tangle no
      def insert_audits_into_db (download_path, auditlog_path):
          # try:
          #     plpy.here?
          # except:
          #     from SQL import sqllib as plpy
          bucket, job = determine_bucket_job("ci-kubernetes-e2e-gci-gce","1232358105564581890")
          download_path, auditlog_path = load_audit_events(bucket, job)
          sql_string = json_to_sql(bucket, job, download_path)
          rv = plpy.execute(sql)
  #+end_src

* Deployment
** configmap
   #+begin_src yaml :tangle ./deployment/configmap.yaml
     apiVersion: v1
     kind: ConfigMap
     metadata:
       name: postgres-configuration
     data:
       POSTGRES_DB: apisnoop
       POSTGRES_USER: apisnoop
       POSTGRES_PASSWORD: s3cretsauc3
       PGDATABASE: apisnoop
       PGUSER: apisnoop
       # APISNOOP_BASELINE_BUCKET: ci-kubernetes-e2e-gci-gce
       # APISNOOP_BASELINE_JOB: 1201280603970867200
   #+end_src
** deployment
   #+begin_src yaml :tangle ./deployment/deployment.yaml
     apiVersion: apps/v1
     kind: Deployment
     metadata:
       name: postgres
     spec:
       replicas: 1
       selector:
         matchLabels:
           io.apisnoop.db: postgres
       template:
         metadata:
           labels:
             io.apisnoop.db: postgres
         spec:
           restartPolicy: Always
           containers:
           - name: postgres
             image: "gcr.io/k8s-staging-apisnoop/postgres:v20200211-0.9.34-1-g24cf96f"
             ports:
             - containerPort: 5432
             livenessProbe:
               exec:
                 command:
                 - "pg_isready"
                 - "-U"
                 - "apisnoop"
               failureThreshold: 5
               periodSeconds: 10
               timeoutSeconds: 5
             env:
             - name: POSTGRES_DB
               valueFrom:
                 configMapKeyRef:
                   name: postgres-configuration
                   key: POSTGRES_DB
             - name: POSTGRES_USER
               valueFrom:
                 configMapKeyRef:
                   name: postgres-configuration
                   key: POSTGRES_USER
             - name: POSTGRES_PASSWORD
               valueFrom:
                 configMapKeyRef:
                   name: postgres-configuration
                   key: POSTGRES_PASSWORD
             - name: PGDATABASE
               valueFrom:
                 configMapKeyRef:
                   name: postgres-configuration
                   key: PGDATABASE
             - name: PGUSER
               valueFrom:
                 configMapKeyRef:
                   name: postgres-configuration
                   key: PGUSER
             # - name: APISNOOP_BASELINE_BUCKET
             #   valueFrom:
             #     configMapKeyRef:
             #       name: postgres-configuration
             #       key: APISNOOP_BASELINE_BUCKET
             # - name: APISNOOP_BASELINE_JOB
             #   valueFrom:
             #     configMapKeyRef:
             #       name: postgres-configuration
             #       key: APISNOOP_BASELINE_JOB
             # - name: APISNOOP_BASELINE_BUCKET
             #   value: ci-kubernetes-e2e-gci-gce
             # - name: APISNOOP_BASELINE_JOB
             #   value: "1201280603970867200"
   #+end_src
** kustomization
   #+begin_src yaml :tangle ./deployment/kustomization.yaml
     apiVersion: kustomize.config.k8s.io/v1beta1
     kind: Kustomization
     resources:
       - deployment.yaml
       - configmap.yaml
       - service.yaml
     # maybe we can use a password generator here?
   #+end_src
** services
   #+begin_src yaml :tangle ./deployment/service.yaml
     apiVersion: v1
     kind: Service
     metadata:
       name: postgres
     spec:
       selector:
         io.apisnoop.db: postgres
       ports:
       - name: "5432"
         port: 5432
         targetPort: 5432
   #+end_src
* Footnotes
** Someday/Maybe later
*** Look into our apisnoop baseline...how are these env's passed and is there an easier way?
